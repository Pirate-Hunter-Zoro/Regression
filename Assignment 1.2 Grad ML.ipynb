{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFXZug066bJX"
      },
      "source": [
        "This project sourced in large part from the University of Waterloo's CS480/680 Introduction to Machine Learning\n",
        "\n",
        "# Upload files in Google Colab\n",
        "If you are running this Jupyter Notebook on Google Colab, uncomment and run this cell to upload the data files (train_inputs.csv, train_targets.csv, test_inputs.csv, test_targets.csv) in the colab virtual machine.  You will be prompted to select files that you would like to upload. \n",
        "\n",
        "If you are running this Jupyter Notebook on your computer, you can delete or ignore this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqBJV_Br4XeI"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# %ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZDpxE4jmFwA"
      },
      "source": [
        "# Import libraries \n",
        "Do not use any other Python library.\n",
        "\n",
        "numpy - Linear algebra library for handling vectors and matrices, collectively processed as numpy arrays.\n",
        "\n",
        "matplotlib - Graphing library for visualizing .\n",
        "\n",
        "sklearn - Machine learning library from which we will source some of our datasets. \n",
        "\n",
        "time - Simple library for timing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_1d0BPfmacB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from time import time\n",
        "\n",
        "# Uncomment below to allow matplotlib to display interactive plots\n",
        "# %matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6keYhcgi7nbf"
      },
      "source": [
        "# Load Datasets\n",
        "\n",
        "These functions load data for classification into RAM for python to use. Some load from files on the device, while some download data from the internet.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "*   **plot**: boolean for whether to plot the data points\n",
        "\n",
        "Outputs:\n",
        "\n",
        "*   **train_inputs**: numpy array of N training data points x M features\n",
        "*   **train_labels**: numpy array of N training labels\n",
        "*   **test_inputs**: numpy array of N' test data points x M features\n",
        "*   **test_labels**: numpy array of N' test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcG5U2lR7utt"
      },
      "outputs": [],
      "source": [
        "def load_synthetic_excel_data(plot=True):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    test_inputs = np.genfromtxt('classification-datasets/synthetic-one-dimension/test_inputs.csv', delimiter=',')\n",
        "    test_inputs = test_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    test_targets = np.genfromtxt('classification-datasets/synthetic-one-dimension/test_targets.csv', delimiter=',')\n",
        "    train_inputs = np.genfromtxt('classification-datasets/synthetic-one-dimension/train_inputs.csv', delimiter=',')\n",
        "    train_inputs = train_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    train_targets = np.genfromtxt('classification-datasets/synthetic-one-dimension/train_targets.csv', delimiter=',')\n",
        "    train_targets = train_targets.astype(int)\n",
        "    test_targets = test_targets.astype(int)\n",
        "    \n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.scatter(train_inputs, train_targets, color='blue', label='Train Data')\n",
        "        plt.scatter(test_inputs, test_targets, color='red', label='Test Data')\n",
        "        plt.xlabel('Input Feature')\n",
        "        plt.ylabel('Target Class')\n",
        "        plt.title('Synthetic One-Dimensional Data')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_scikit_cancer_data(plot=True):\n",
        "    # Load the breast cancer diagnostic dataset from scikit-learn\n",
        "    cancer = load_breast_cancer()\n",
        "    inputs = cancer.data\n",
        "    targets = cancer.target\n",
        "    test_inputs = inputs[:100]\n",
        "    test_targets = targets[:100]\n",
        "    train_inputs = inputs[100:]\n",
        "    train_targets = targets[100:]\n",
        "    \n",
        "    # Normalize inputs\n",
        "    train_inputs = (train_inputs - np.mean(train_inputs, axis=0)) / np.std(train_inputs, axis=0)\n",
        "    test_inputs = (test_inputs - np.mean(test_inputs, axis=0)) / np.std(test_inputs, axis=0)\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.scatter(train_inputs[:, 0], train_inputs[:, 1], c=train_targets, cmap='viridis', edgecolors='k', s=20)\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Feature 2')\n",
        "        plt.title('Breast Cancer Diagnostic Data')\n",
        "        plt.colorbar(label='Target Class')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_scikit_49_digits_data(plot=True):\n",
        "    # Load the optical digits dataset from scikit-learn\n",
        "    digits = load_digits()\n",
        "    inputs = digits.data\n",
        "    targets = digits.target\n",
        "    # Filter just the 4's and 9's\n",
        "    inputs = inputs[(targets==4) | (targets==9)]\n",
        "    targets = targets[(targets==4) | (targets==9)]\n",
        "    targets[targets==4] = 0\n",
        "    targets[targets==9] = 1\n",
        "    # Split into training and testing sets\n",
        "    test_inputs = inputs[:100]\n",
        "    test_targets = targets[:100]\n",
        "    train_inputs = inputs[100:]\n",
        "    train_targets = targets[100:]\n",
        "    # Rescale inputs to the range [0, 1]\n",
        "    train_inputs = (train_inputs) / 16\n",
        "    test_inputs = (test_inputs) / 16\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        # Plot example digits\n",
        "        inputs_reshaped = train_inputs.reshape(-1, 8, 8)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for i in range(10):\n",
        "            plt.subplot(3, 4, i + 1)\n",
        "            plt.imshow(inputs_reshaped[i], cmap='gray')\n",
        "            plt.title(f'Handwritten {4 if train_targets[i]==0 else 9}')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_scikit_mini_digits_data(plot=True):\n",
        "    # Load the optical digits dataset from scikit-learn\n",
        "    digits = load_digits()\n",
        "    inputs = digits.data\n",
        "    targets = digits.target\n",
        "    test_inputs = inputs[:100]\n",
        "    test_targets = targets[:100]\n",
        "    train_inputs = inputs[100:]\n",
        "    train_targets = targets[100:]\n",
        "    # Rescale inputs to the range [0, 1]\n",
        "    train_inputs = train_inputs / 16\n",
        "    test_inputs = test_inputs / 16\n",
        "\n",
        "    train_sample_count = 100\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        # Plot example digits\n",
        "        inputs_reshaped = train_inputs.reshape(-1, 8, 8)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for i in range(10):\n",
        "            plt.subplot(3, 4, i + 1)\n",
        "            plt.imshow(inputs_reshaped[i], cmap='gray')\n",
        "            plt.title(f'Handwritten {train_targets[i]}')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function: softmax\n",
        "\n",
        "This function implements the vectorized logistic softmax function.\n",
        "\n",
        "Input:\n",
        "\n",
        "*   **input**: vector of inputs (N x K numpy array of floats)\n",
        "\n",
        "Output:\n",
        "*   **output**: vector of outputs (N x K numpy array of floats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(input):\n",
        "    # Make sure you use a numerically stable solution\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwLo3p4f8bTa"
      },
      "source": [
        "# Function: predict_logistic_regression\n",
        "\n",
        "This function uses a vector of weights to make predictions for a set of inputs.  \n",
        "The prediction for each data point is a distribution over the labels. \n",
        "Assume that there is one column for each class.\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input data points for which we want to make a prediction (numpy array of N data points x M+1 features)\n",
        "*   **weights**: vector of weights (numpy array of M+1 x K weights)\n",
        "\n",
        "Output:\n",
        "*   **predicted_probabilities**: matrix of predicted probabilities (numpy array of N data points x K labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX04_wClRqkV"
      },
      "outputs": [],
      "source": [
        "def predict_logistic_regression(inputs, weights):\n",
        "    # Compute the linear combination of inputs and weights\n",
        "    # Apply the softmax function to get predicted probabilities\n",
        "    return predicted_probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmfPN7K0RtQ5"
      },
      "source": [
        "# Function eval_logistic_regression\n",
        "\n",
        "This function evaluates a set of predictions by computing the negative log probabilities of the labels and the accuracy (percentage of correctly predicted labels).  Assume that there are only two possible labels {0,1}.  A data point is correctly labeled when the probability of the target label is >= 0.5.\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input data points for which we will evaluate the predictions (numpy array of N data points x M+1 features)\n",
        "*   **weights**: vector of weights (numpy array of M+1 x K weights)\n",
        "*   **labels**: vector of target labels associated with the inputs (numpy array of N x K labels)\n",
        "\n",
        "Outputs:\n",
        "*   **neg_log_prob**: negative log probability of the set of predictions (float)\n",
        "*   **accuracy**: percentage of correctly labeled data points (float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC14LEsvTxbu"
      },
      "outputs": [],
      "source": [
        "def eval_logistic_regression(inputs, weights, labels):\n",
        "    # Compute the linear combination of inputs and weights\n",
        "    # Compute loss from linear combination using numerically stable computation\n",
        "    # Apply the softmax function on linear combination to get predicted probabilities\n",
        "    # Compute accuracy\n",
        "    return losses, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7hZ4XVP4U4y"
      },
      "source": [
        "# Function: initialize_weights\n",
        "\n",
        "This function initializes the weights uniformly at random in the interval [-0.01,0.01]\n",
        "\n",
        "Input:\n",
        "*   **n_weights**: # of weights to be initialized (integer)\n",
        "*   **n_classes**: # of type classes to classify into (integer)\n",
        "\n",
        "Output:\n",
        "*   **random_weights**: vector of weights (M x K numpy array of floats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OjhevpV5FBg"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(n_features, n_classes=2):\n",
        "    return random_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMAzC5xXT0H-"
      },
      "source": [
        "# Function train_logistic_regression_gradient\n",
        "\n",
        "This function optimizes a set of weights for logistic regression based on a training set using the gradient descent method.\n",
        "\n",
        "Inputs:\n",
        "*   **train_inputs**: matrix of input training points (numpy array of N data points x M+1 features)\n",
        "*   **train_labels**: vector of labels associated with the inputs (numpy array of N labels)\n",
        "*   **eta_hyperparam**: learning rate for the gradient descent optimization (scalar)\n",
        "*   **lambda_hyperparam**: lambda hyperparameter used to adjust the overall degree of regularization (scalar)\n",
        "*   **lasso_ridge_ratio**: hyperparameter used to adjust the amount of lasso (L1) regularization versus ridge (L2) regularization (scalar 0-1)\n",
        "*   **num_iterations**: number of iterations of the gradient descent algorithm to perform\n",
        "\n",
        "Output:\n",
        "*   **weights**: vector of weights that have been optimized (numpy array of M+1 x K weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DkzoT5QVy41"
      },
      "outputs": [],
      "source": [
        "def train_logistic_regression_gradient_descent(train_inputs, train_labels, eta_hyperparam=0.01, lambda_hyperparam=0.0, lasso_ridge_ratio=1.0, num_iters=1000):\n",
        "    # Initialize weights\n",
        "    for it in range(num_iters):\n",
        "        # Compute the predicted probabilities\n",
        "        # Compute the gradient\n",
        "        # Apply L1 regularization (lasso) to the gradient\n",
        "        # Apply L2 regularization (ridge) except for the bias term\n",
        "        # Update the weights\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIbLxX7V2DW"
      },
      "source": [
        "# Cross validation functions\n",
        "\n",
        "These functions perform k-fold cross validation to search for optimal hyperparameters.\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input points (numpy array of N data points by M+1 features)\n",
        "*   **labels**: vector of labels associated with the inputs (numpy array of N labels)\n",
        "*   **k_folds**: # of folds in cross-validation (integer)\n",
        "*   **lasso_ridge_ratio**: hyperparaemter used to adjust the amount of LASSO (L1) regularization versus ridge (L2) regularization (scalar 0-1)\n",
        "*   **num_iterations**: number of iterations of the gradient descent algorithm to perform (integer)\n",
        "\n",
        "Outputs:\n",
        "*   **neg_log_probabilities**: dict of hyperparameter to negative log probabilities for the corresponding hyperparameter (float)\n",
        "*   **accuracies**: dict of hyperparameter to average accuracy for the corresponding hyperparaemter (float)\n",
        "\n",
        "# Function cross_validation_eta\n",
        "\n",
        "This function performs k-fold cross validation to determine the best eta hyperparameter for logistic regression.\n",
        "\n",
        "Additional Inputs:\n",
        "*   **eta_hyperparams**: list of learning rate hyperparameters where each hyperparameter is a different eta value (list of floats)\n",
        "*   **lambda_hyperparam**: lambda hyperparameter used to adjust the overall degree of regularization (scalar)\n",
        "\n",
        "# Function cross_validation_lambda\n",
        "\n",
        "This function performs k-fold cross validation to determine the best lambda hyperparameter for logistic regression.\n",
        "\n",
        "Additional Inputs:\n",
        "*   **eta_hyperparam**: learning rate hyperparameters where each hyperparameter is a different eta value (scalar)\n",
        "*   **lambda_hyperparams**: list of lambda hyperparameter used to adjust the overall degree of regularization (list of floats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZzoiZxLZMcV"
      },
      "outputs": [],
      "source": [
        "def cross_validation_eta(inputs, labels, k_folds, eta_hyperparams, lambda_hyperparam, lasso_ridge_ratio, num_iterations):\n",
        "    # Shuffle the data\n",
        "    # Perform k-fold cross-validation\n",
        "    for i, eta_hyperparam in enumerate(eta_hyperparams):\n",
        "        for fold in range(k_folds):\n",
        "            # Split the data into training and validation sets\n",
        "            # Train the model\n",
        "            # Evaluate on the validation set\n",
        "    return neg_log_probabilities, accuracies\n",
        "\n",
        "def cross_validation_lambda(inputs, labels, k_folds, eta_hyperparam, lambda_hyperparams, lasso_ridge_ratio, num_iterations):\n",
        "    # Shuffle the data\n",
        "    # Perform k-fold cross-validation\n",
        "    for i, lambda_hyperparam in enumerate(lambda_hyperparams):\n",
        "        for fold in range(k_folds):\n",
        "            # Split the data into training and validation sets\n",
        "            # Train the model\n",
        "            # Evaluate on the validation set\n",
        "            neg_log_prob, accuracy = eval_logistic_regression(val_inputs, weights, val_labels)\n",
        "            fold_neg_log_probs.append(neg_log_prob)\n",
        "            fold_accuracies.append(accuracy)\n",
        "    return neg_log_probabilities, accuracies    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ah5AAayZfVU"
      },
      "source": [
        "# Function: plot_logistic_regression_neg_log_probabilities\n",
        "\n",
        "Function that plots the negative log probabilities for different lambda values (hyperparameters) in logistic regression based on cross validation\n",
        "\n",
        "Inputs:\n",
        "*   **neg_log_probabilities**: vector of negative log probabilities for the corresponding hyperparameters (numpy array of floats)\n",
        "*   **accuracies**: vector of fractions of correct predictions for the corresponding hyperparameters (numpy array of floats)\n",
        "*   **hyperparams**: list of hyperparameters where each hyperparameter is a different lambda value (list of floats)\n",
        "\n",
        "# Function: plot_decision_boundary\n",
        "\n",
        "Function that plots the decision boundary for 1D input data\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input points (numpy array of N data points by M+1 features)\n",
        "*   **labels**: vector of labels associated with the inputs (numpy array of N data points by K labels)\n",
        "*   **weights**: vector of trained weights (numpy array of M+1 x K weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh9qZuzMatsZ"
      },
      "outputs": [],
      "source": [
        "def plot_eta_vs_metrics(neg_log_probabilities,accuracies,hyperparams):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(hyperparams,neg_log_probabilities)\n",
        "    plt.ylabel('negative log probability')\n",
        "    plt.xlabel('eta')\n",
        "    plt.grid()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(hyperparams,accuracies)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('eta')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_lambda_vs_metrics(neg_log_probabilities,accuracies,hyperparams):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(hyperparams,neg_log_probabilities)\n",
        "    plt.ylabel('negative log probability')\n",
        "    plt.xlabel('lambda')\n",
        "    plt.grid()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(hyperparams,accuracies)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('lambda')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_decision_boundary(inputs, labels, weights):\n",
        "    # Check if input is 2 dimensional\n",
        "    if len(inputs[0]) != 2:\n",
        "        print(\"plot_decision_boundary only accepts 1-dimensional input + bias.\")\n",
        "        return\n",
        "    probs = predict_logistic_regression(inputs, weights)\n",
        "    x_values = np.linspace(-5, 15, 100)\n",
        "    y_probs = predict_logistic_regression(np.column_stack((np.ones(x_values.shape), x_values)), weights)\n",
        "    y_values = y_probs[:, 0]  # Get the probabilities for class 1\n",
        "    # 2d plot\n",
        "    plt.figure()\n",
        "    # Draw logistic curve under the data\n",
        "    plt.plot(x_values, y_values, color='red', label='Logistic Curve', zorder=1)\n",
        "    plt.scatter(inputs[:, 1], probs[:,0], c=labels[:,0], cmap='viridis', edgecolors='k', s=20, zorder=2)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Predicted Probability of Class 1')\n",
        "    plt.colorbar(label='True Class')\n",
        "    plt.title('Test Data with Predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s21LRP5Qb3m8"
      },
      "source": [
        "# Main Logistic Regression code\n",
        "\n",
        "Use this section to train and test your models. You will do the following:\n",
        "\n",
        "Load data.\n",
        "\n",
        "Use k-fold cross validation to find the best lambda value for logistic regression.\n",
        "\n",
        "Plot the negative log probabilities for different lambda values.\n",
        "\n",
        "Test logistic regression on full training data with the best lambda value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is one way you can test multiple at once, but implement it however you like\n",
        "for dataset_name, dataset_info in datasets.items():\n",
        "    for alg_name, alg in algorithms.items():\n",
        "        load_dataset = dataset_info[\"load\"]\n",
        "        lambda_hyperpraram = dataset_info[\"lambda\"]\n",
        "        eta_hyperparams = dataset_info[\"etas\"]\n",
        "        run_train = alg[\"train\"]\n",
        "        run_validation = alg[\"validation\"]\n",
        "        regularization = alg[\"regularization\"]\n",
        "        \n",
        "        # Convert classes to one-hot encoding. You may assume labels are 0-indexed integers\n",
        "        # Add bias term (1) at the front of each data point\n",
        "        # Run cross validation\n",
        "        # Find best and worst parameters\n",
        "        # Plot results\n",
        "        # train and evaluate with best parameter"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "curriculum",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
