{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFXZug066bJX"
      },
      "source": [
        "This project sourced in large part from the University of Waterloo's CS480/680 Introduction to Machine Learning\n",
        "\n",
        "# Upload files in Google Colab\n",
        "If you are running this Jupyter Notebook on Google Colab, uncomment and run this cell to upload the data files (train_inputs.csv, train_targets.csv, test_inputs.csv, test_targets.csv) in the colab virtual machine.  You will be prompted to select files that you would like to upload. \n",
        "\n",
        "If you are running this Jupyter Notebook on your computer, you can delete or ignore this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqBJV_Br4XeI"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# %ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZDpxE4jmFwA"
      },
      "source": [
        "# Import libraries \n",
        "Do not use any other Python library.\n",
        "\n",
        "numpy - Linear algebra library for handling vectors and matrices, collectively processed as numpy arrays.\n",
        "\n",
        "matplotlib - Graphing library for visualizing .\n",
        "\n",
        "sklearn - Machine learning library from which we will source some of our datasets. \n",
        "\n",
        "time - Simple library for timing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_1d0BPfmacB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from time import time\n",
        "\n",
        "# Uncomment below to allow matplotlib to display interactive plots in the notebook\n",
        "# %matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6keYhcgi7nbf"
      },
      "source": [
        "# Load Datasets\n",
        "\n",
        "These functions load data for regression into RAM for python to use. Some load from files on the device, while some download data from the internet.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "*   **plot**: boolean for whether to plot the data points\n",
        "\n",
        "Outputs:\n",
        "\n",
        "*   **train_inputs**: numpy array of N training data points x M features\n",
        "*   **train_targets**: numpy array of N training targets\n",
        "*   **test_inputs**: numpy array of N' test data points x M features\n",
        "*   **test_targets**: numpy array of N' test targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcG5U2lR7utt"
      },
      "outputs": [],
      "source": [
        "def load_synthetic_excel_data(plot=True):\n",
        "    # Load synthetic data from CSV files\n",
        "    test_inputs = np.genfromtxt('regression-datasets/synthetic-one-dimension/test_inputs.csv', delimiter=',')\n",
        "    test_inputs = test_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    test_targets = np.genfromtxt('regression-datasets/synthetic-one-dimension/test_targets.csv', delimiter=',')\n",
        "    train_inputs = np.genfromtxt('regression-datasets/synthetic-one-dimension/train_inputs.csv', delimiter=',')\n",
        "    train_inputs = train_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    train_targets = np.genfromtxt('regression-datasets/synthetic-one-dimension/train_targets.csv', delimiter=',')\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.scatter(train_inputs, train_targets, color='blue', label='Train Data')\n",
        "        plt.scatter(test_inputs, test_targets, color='red', label='Test Data')\n",
        "        plt.xlabel('Input Feature')\n",
        "        plt.ylabel('Target Value')\n",
        "        plt.title('Synthetic One-Dimensional Data')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_synthetic_quadratic_excel_data(plot=True): \n",
        "    # Load synthetic data from CSV files\n",
        "    test_inputs = np.genfromtxt('regression-datasets/synthetic-one-dimension-quad/test_inputs.csv', delimiter=',')\n",
        "    test_inputs = test_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    test_targets = np.genfromtxt('regression-datasets/synthetic-one-dimension-quad/test_targets.csv', delimiter=',')\n",
        "    train_inputs = np.genfromtxt('regression-datasets/synthetic-one-dimension-quad/train_inputs.csv', delimiter=',')\n",
        "    train_inputs = train_inputs.reshape(-1, 1)  # Ensure it's a 2D array\n",
        "    train_targets = np.genfromtxt('regression-datasets/synthetic-one-dimension-quad/train_targets.csv', delimiter=',')\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.scatter(train_inputs, train_targets, color='blue', label='Train Data')\n",
        "        plt.scatter(test_inputs, test_targets, color='red', label='Test Data')\n",
        "        plt.xlabel('Input Feature')\n",
        "        plt.ylabel('Target Value')\n",
        "        plt.title('Synthetic One-Dimensional Data')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_sklearn_california_housing(plot=True): \n",
        "    # Load the California housing dataset from scikit-learn\n",
        "    california_housing = fetch_california_housing()\n",
        "    train_inputs = california_housing.data[2000:]\n",
        "    train_targets = california_housing.target[2000:]\n",
        "    test_inputs = california_housing.data[0:2000]\n",
        "    test_targets = california_housing.target[0:2000]\n",
        "\n",
        "    # Center and normalize the data\n",
        "    mean = np.mean(train_inputs, axis=0)\n",
        "    stdev = np.std(train_inputs, axis=0)\n",
        "    train_inputs = (train_inputs - mean) / stdev\n",
        "    test_inputs = (test_inputs - mean) / stdev\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.scatter(train_inputs[:, 0], train_targets, color='blue', label='Train Data')\n",
        "        plt.scatter(test_inputs[:, 0], test_targets, color='red', label='Test Data')\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Target Value')\n",
        "        plt.title('California Housing Data')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "def load_sklearn_49_digits_data(plot=True):\n",
        "    # Load the optical digits dataset from scikit-learn\n",
        "    digits = load_digits()\n",
        "    inputs = digits.data\n",
        "    targets = digits.target\n",
        "    # Filter just the 4's and 9's\n",
        "    inputs = inputs[(targets==4) | (targets==9)]\n",
        "    targets = targets[(targets==4) | (targets==9)]\n",
        "    targets[targets==4] = 0\n",
        "    targets[targets==9] = 1\n",
        "    # Split into training and testing sets\n",
        "    test_inputs = inputs[:100]\n",
        "    test_targets = targets[:100]\n",
        "    train_inputs = inputs[100:]\n",
        "    train_targets = targets[100:]\n",
        "    # Rescale inputs to the range [0, 1]\n",
        "    train_inputs = (train_inputs) / 16\n",
        "    test_inputs = (test_inputs) / 16\n",
        "\n",
        "    train_sample_count = 20\n",
        "    test_sample_count = 100\n",
        "    np.random.seed(42)\n",
        "    train_samples = np.random.choice(train_inputs.shape[0], train_sample_count)\n",
        "    test_samples = np.random.choice(test_inputs.shape[0], test_sample_count)\n",
        "    train_inputs = train_inputs[train_samples]\n",
        "    train_targets = train_targets[train_samples]\n",
        "    test_inputs = test_inputs[test_samples]\n",
        "    test_targets = test_targets[test_samples]\n",
        "\n",
        "    if plot:\n",
        "        # Plot example digits\n",
        "        inputs_reshaped = train_inputs.reshape(-1, 8, 8)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for i in range(10):\n",
        "            plt.subplot(3, 4, i + 1)\n",
        "            plt.imshow(inputs_reshaped[i], cmap='gray')\n",
        "            plt.title(f'Handwritten {4 if train_targets[i]==0 else 9}')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return train_inputs, train_targets, test_inputs, test_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwLo3p4f8bTa"
      },
      "source": [
        "# Function: predict_linear_regression\n",
        "\n",
        "This function uses a vector of weights to make predictions for a set of inputs.\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input data points for which we want to make a prediction (numpy array of N data points x M+1 features)\n",
        "*   **weights**: vector of weights (numpy array of M+1 weights)\n",
        "\n",
        "Output:\n",
        "*   **predicted_values**: vector of predicted values (numpy array of N floats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX04_wClRqkV"
      },
      "outputs": [],
      "source": [
        "def predict_linear_regression(inputs, weights):\n",
        "    return predicted_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmfPN7K0RtQ5"
      },
      "source": [
        "# Function eval_linear_regression\n",
        "\n",
        "This function evaluates a set of predictions by computing the mean squared error with respect to the targets\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input data points for which we will evaluate the predictions (numpy array of N data points x M+1 features)\n",
        "*   **weights**: vector of weights (numpy array of M+1 weights)\n",
        "*   **targets**: vector of targets associated with the inputs (numpy array of N targets)\n",
        "\n",
        "Output:\n",
        "*   **mean_squared_error**: mean squared error between the predicted values and the targets (scalar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC14LEsvTxbu"
      },
      "outputs": [],
      "source": [
        "def eval_linear_regression(inputs, weights, targets):\n",
        "    # predict output for given inputs\n",
        "    # calculate mean squared error loss\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMAzC5xXT0H-"
      },
      "source": [
        "# Function train_linear_regression_normal_equation\n",
        "\n",
        "This function optimizes a set of weights for linear regression based on a training set using the normal equation method.\n",
        "\n",
        "Inputs:\n",
        "*   **train_inputs**: matrix of input training points (numpy array of N data points x M+1 features)\n",
        "*   **train_targets**: vector of targets associated with the inputs (numpy array of N targets)\n",
        "*   **lambda_hyperparam**: lambda hyperparameter used to adjust the importance of the ridge (L2) regularizer (scalar)\n",
        "\n",
        "Output:\n",
        "*   **weights**: vector of weights that have been optimized (numpy array of M+1 weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function train_linear_regression_gradient_descent\n",
        "\n",
        "This function optimizes a set of weights for linear regression based on a training set using the gradient descent method.\n",
        "\n",
        "Inputs:\n",
        "*   **train_inputs**: matrix of input training points (numpy array of N data points x M+1 features)\n",
        "*   **train_targets**: vector of targets associated with the inputs (numpy array of N targets)\n",
        "*   **eta_hyperparam**: learning rate for the gradient descent optimization (scalar)\n",
        "*   **lambda_hyperparam**: lambda hyperparameter used to adjust the overall degree of regularization (scalar)\n",
        "*   **lasso_ridge_ratio**: hyperparameter used to adjust the amount of lasso (L1) regularization versus ridge (L2) regularization (scalar 0-1)\n",
        "*   **num_iterations**: number of iterations of the gradient descent algorithm to perform (integer)\n",
        "\n",
        "Output:\n",
        "*   **weights**: vector of weights that have been optimized (numpy array of M+1 weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DkzoT5QVy41"
      },
      "outputs": [],
      "source": [
        "def train_linear_regression_normal_equation(train_inputs, train_targets, lambda_hyperparam):\n",
        "    # Compute the weights using the normal equation with regularization\n",
        "    # Do not regularize the bias term\n",
        "    # The normal equation is w = (X.T X).(-1) X.T y\n",
        "    return weights\n",
        "\n",
        "def train_linear_regression_gradient_descent(train_inputs, train_targets, eta_hyperparam=0.01, lambda_hyperparam=0.0, lasso_ridge_ratio=1.0, num_iterations=1000):\n",
        "    # Initialize weights\n",
        "    for it in range(num_iterations):\n",
        "        # Compute the gradient\n",
        "        # Apply L1 regularization (Lasso) except for the bias term\n",
        "        # Apply L2 regularization (Ridge) except for the bias term\n",
        "        # Update the weights\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross validation functions\n",
        "\n",
        "These functions perform k-fold cross validation to search for optimal model hyperparameters.\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input points (numpy array of N data points by M+1 features)\n",
        "*   **targets**: vector of targets associated with the inputs (numpy array of N targets)\n",
        "*   **k_folds**: # of folds in cross-validation (integer)\n",
        "\n",
        "Outputs:\n",
        "*   **mean_squared_errors**: dict of hyperparameters to vector of mean squared errors for the corresponding hyperparameter (float)\n",
        "\n",
        "# Function cross_validation_lambda_normal_equation\n",
        "\n",
        "This function performs k-fold cross validation to determine the best lambda hyperparameter in normal equation linear regression.\n",
        "\n",
        "Additional Inputs:\n",
        "*   **lambda_hyperparams**: list of lambda hyperparameter used to adjust the overall degree of regularization (list of floats)\n",
        "\n",
        "# Function cross_validation_eta_gradient_descent\n",
        "\n",
        "This function performs k-fold cross validation to determine the best eta parameter hyperparameter in gradient descent linear regression.\n",
        "\n",
        "Additional Inputs:\n",
        "*   **eta_hyperparams**: list of learning rate hyperparameters where each hyperparameter is a different eta value (list of floats)\n",
        "*   **lambda_hyperparam**: lambda hyperparameter used to adjust the overall degree of regularization (scalar)\n",
        "*   **lasso_ridge_ratio**: hyperparameter used to adjust the amount of lasso (L1) regularization versus ridge (L2) regularization (scalar 0-1)\n",
        "*   **num_iterations**: number of iterations of the gradient descent algorithm to perform (integer)\n",
        "\n",
        "# Function cross_validation_lambda_gradient_descent\n",
        "\n",
        "This function performs k-fold cross validation to determine the best lambda hyperparameter in gradient descent linear regression.\n",
        "\n",
        "Additional Inputs\n",
        "*   **eta_hyperparam**: learning rate hyperparameters where each hyperparameter is a different eta value (scalar)\n",
        "*   **lambda_hyperparams**: list of lambda hyperparameter used to adjust the overall degree of regularization (list of floats)\n",
        "*   **lasso_ridge_ratio**: hyperparameter used to adjust the amount of lasso (L1) regularization versus ridge (L2) regularization (scalar 0-1)\n",
        "*   **num_iterations**: number of iterations of the gradient descent algorithm to perform (integer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation_lambda_normal_equation(inputs, targets, k_folds, lambda_hyperparams):\n",
        "    # Shuffle the data\n",
        "    # Perform k-fold cross-validation\n",
        "    for lambda_hyperparam in lambda_hyperparams:\n",
        "        for fold in range(k_folds):\n",
        "            # Split the data into training and validation sets\n",
        "            # Train the model\n",
        "            # Evaluate the model\n",
        "    return dict(zip(lambda_hyperparams, mean_squared_errors))\n",
        "\n",
        "def cross_validation_eta_gradient_descent(inputs, targets, k_folds, eta_hyperparams, lambda_hyperparam, lasso_ridge_ratio, num_iterations):\n",
        "    # Shuffle the data\n",
        "    # Perform k-fold cross-validation\n",
        "    for eta_hyperparam in eta_hyperparams:\n",
        "        for fold in range(k_folds):\n",
        "            # Split the data into training and validation sets\n",
        "            # Train the model\n",
        "            # Evaluate the model\n",
        "    return dict(zip(eta_hyperparams, mean_squared_errors))\n",
        "\n",
        "def cross_validation_lambda_gradient_descent(inputs, targets, k_folds, eta_hyperparam, lambda_hyperparams, lasso_ridge_ratio, num_iterations):\n",
        "    # Shuffle the data\n",
        "    # Perform k-fold cross-validation\n",
        "    for lambda_hyperparam in lambda_hyperparams:\n",
        "        for fold in range(k_folds):\n",
        "            # Split the data into training and validation sets\n",
        "            # Train the model\n",
        "            # Evaluate the model\n",
        "    return dict(zip(lambda_hyperparams, mean_squared_errors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ah5AAayZfVU"
      },
      "source": [
        "# Function: plot_linear_regression_mean_squared_errors\n",
        "\n",
        "Function that plots the mean squared errors for different lambda values (hyperparameters) in linear regression based on cross validation\n",
        "\n",
        "Inputs:\n",
        "*   **mean_squared_errors**: vector of mean squared errors for the corresponding hyperparameters (numpy array of floats)\n",
        "*   **hyperparams**: list of hyperparameters where each hyperparameter is a different lambda value (list of floats)\n",
        "\n",
        "# Function: plot_linear_predictor\n",
        "\n",
        "Function that plots the linear prediction function for 1D feature spaces\n",
        "\n",
        "Inputs:\n",
        "*   **inputs**: matrix of input points (numpy array of N data points by M+1 features)\n",
        "*   **targets**: vector of targets associated with the inputs (numpy array of N targets)\n",
        "*   **weights_dict**: dictionary of names to vector of weights (numpy array of M+1 weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh9qZuzMatsZ"
      },
      "outputs": [],
      "source": [
        "def plot_lambda_vs_mean_squared_errors(mean_squared_errors, hyperparams):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(hyperparams,mean_squared_errors)\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xlabel(\"lambda\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_eta_vs_mean_squared_errors(mean_squared_errors, hyperparams):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(hyperparams,mean_squared_errors)\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xlabel(\"eta\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_linear_predictor(test_inputs, test_targets, weights_dict):\n",
        "    if len(test_inputs[0]) not in [2, 3]:\n",
        "        print(\"plot_linear_predictor only accepts 1D or 2D data + bias\")\n",
        "        return\n",
        "\n",
        "    if len(test_inputs[0]) == 2:\n",
        "    # 2d plot\n",
        "        plt.figure()\n",
        "        plt.scatter(test_inputs[:, 1], test_targets, color='blue', label='Actual Values')\n",
        "        for name, weight in weights_dict.items():\n",
        "            plt.plot(test_inputs[:, 1], predict_linear_regression(test_inputs, weight), label=name, marker=\"o\", linestyle=\"-\")\n",
        "        plt.xlabel('Feature Value')\n",
        "        plt.ylabel('Target Value')\n",
        "        plt.title('Linear Regression Predictions vs Actual Values')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    if len(test_inputs[0]) == 3:\n",
        "    # 3d plot\n",
        "        from mpl_toolkits.mplot3d import Axes3D\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(test_inputs[:, 1], test_inputs[:, 2], test_targets, color='blue', label='Actual Values')\n",
        "        for name, weight in weights_dict.items():\n",
        "            ax.scatter(test_inputs[:, 1], test_inputs[:, 2], predict_linear_regression(test_inputs, weight), color='red', label=name)\n",
        "        ax.set_xlabel('Feature 1')\n",
        "        ax.set_ylabel('Feature 2')\n",
        "        ax.set_zlabel('Target Value')\n",
        "        ax.set_title('3D Linear Regression Predictions vs Actual Values')\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "def digits_accuracy(predictions, targets):\n",
        "    predictions = predictions >= 0.5\n",
        "    accuracy = np.mean(predictions == targets)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s21LRP5Qb3m8"
      },
      "source": [
        "# Main Linear Regression code\n",
        "\n",
        "Use this section to train and test your models. You will do the following:\n",
        "\n",
        "Load data.\n",
        "\n",
        "Use k-fold cross validation to find the best lambda value for linear regression.\n",
        "\n",
        "Plot mean squared errors for different lambda values.\n",
        "\n",
        "Test linear regression trained on full training data with the best lambda value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njlK2bf7sycQ"
      },
      "outputs": [],
      "source": [
        "# This is one way you can test multiple at once, but implement it however you like\n",
        "for dataset_name, dataset_info in datasets.items():\n",
        "    for alg_name, alg in algorithms.items():\n",
        "        load_dataset = dataset_info[\"load\"]\n",
        "        lambda_hyperpraram = dataset_info[\"lambda\"]\n",
        "        eta_hyperparams = dataset_info[\"etas\"]\n",
        "        run_train = alg[\"train\"]\n",
        "        run_validation = alg[\"validation\"]\n",
        "        regularization = alg[\"regularization\"]\n",
        "\n",
        "        # Add bias term (1) to the from of each data point\n",
        "        # Run cross validation\n",
        "        # Find best and worst parameters\n",
        "        # plot results\n",
        "        # train and evaluate with best parameter"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "curriculum",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
